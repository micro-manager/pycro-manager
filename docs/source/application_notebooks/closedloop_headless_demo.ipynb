{
 "cells": [
  {
   "source": [
    "# Closed-loop acquisition and perturbation with pycro-manager\n",
    "\n",
    "### an example of closed-loop experimentation enabled by pycro-manager\n",
    "\n",
    "When imaging live biological samples, we often have specific features of cellular activity we are interested in, such as a pattern of neural activity or stage in the cell cycle. We can interrogate these dynamics with closed-loop (CL) experimental design. CL perturbations are triggered by signals derived from data acquired from the sample itself during a live recording session. Recent advancements in computing allow experimenters to coduct closed-loop experiments, which will deeply influence optical physiology, allowing realtime adaptation to animal state, enforcement of physiological constraints on evoked patterns, calibrated control with cellular resolution, and a variety of important experimental controls that were previously inaccessible (Grosenick, Marshel, and Deisseroth 2016 Neuron). Specifically, CL experiments:\n",
    "\n",
    "* ensure perturbation occurs during statistically rare conditions\n",
    "* allow online tuning of optogenetic inputs in vivo (to achieve specific output parameters)\n",
    "* allow online system identification / modeling of neural circuits (i.e. causally establish functional circuit architecture)\n",
    "* steer the system into desired or otherwise non-observable states\n",
    "* eliminate off-target effects of non-closed-loop perturbations\n",
    "* reduce variability of system state at time of stimulus onset\n",
    "\n",
    "In this example we use features of pycro-manager which enable closed-loop experimentation. Specifically we perform some canonical image processing (template filtering with 2d gaussian kernel, thresholding, median filtering), then find local peaks, then take a window of pixel values around each peak. We use these pixel values to trigger our arbitrary \"stimulus\" function which can e.g. change optical settings on the microscope, call a separate program, etc.\n",
    "\n",
    "  \n",
    "\n",
    "Here we use snap_image() to acquire our images for readability and to show an example of headless pycromanager acquisition. Alternatively one could use pycro-manager Acquisitions to run our closed-loop experiment. We also leverage a few neat tricks:\n",
    "\n",
    "* we strobe our imaging acquisition by introducing a small delay between images. This makes snap_image() timing an order of magnitude more accurate, and reflects a common imaging condition for perturbative experiments, and gives our closed-loop processing algorithm time to perform computation.\n",
    "* we use the python package numba to just-in-time compile our closed-loop computation into LLVM intermediate representation. This affords an order-of-magnitude speedup, as numba-compiled numerical algorithms can allow Python code to approach the speeds of C.\n",
    "\n",
    "By Raymond L. Dunn, the FOCO Lab, UC San Francisco\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### code\n",
    "load pycro-manager objects and parameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple single image acquisition example with snap\n",
    "from pycromanager import Bridge\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "#### Setup ####\n",
    "bridge = Bridge()\n",
    "core = bridge.get_core()\n",
    "\n",
    "#### imaging settings\n",
    "exposure = 20\n",
    "num_frames_to_capture = 100\n",
    "core.set_exposure(exposure)\n",
    "\n",
    "#### strobe settings\n",
    "# by enforcing a strobe (a small delay between acquisitions), our snap_image acquisition framerate becomes an order of magnitude more precise (as of 20201006)\n",
    "interframe_interval = 50\n",
    "assert interframe_interval > exposure\n",
    "\n",
    "#### holder variables for images, model values, and processing timestamps\n",
    "frames = []\n",
    "model = []\n",
    "acq_timestamps = []\n",
    "process_timestamps = []\n"
   ]
  },
  {
   "source": [
    "define a helpful and fast image processing class for, in this demo, extracting local peaks within an image"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from numba import jit\n",
    "\n",
    "# image processing convenience class\n",
    "# you might have to install some other python dependencies, but this should be clear while trying to run demo\n",
    "class ImageProcessor:\n",
    "    def __init__(self, ops=None):\n",
    "\n",
    "        if ops is None:\n",
    "            ops = build_ops()\n",
    "\n",
    "        # store ops\n",
    "        self.ops = ops\n",
    "\n",
    "        # helper fxn to initialize a template filter for\n",
    "        self.template_filter = gaussian_2d(\n",
    "            1 + 6 * self.ops[\"template_filter_width\"], self.ops[\"template_filter_width\"]\n",
    "        )\n",
    "\n",
    "    def set_ops(self, ops):\n",
    "        \"\"\" helper wrapper to change imageprocessor params, to be called if you want to update default parameters for image processing after object instantiation \"\"\"\n",
    "\n",
    "        self.ops = ops\n",
    "\n",
    "    def segmentchunk(self, frame):\n",
    "        \"\"\" wrapper function to take a frame, segment local peaks, and merge adjacent peaks \"\"\"\n",
    "\n",
    "        # image preprocessing\n",
    "        filt_frame = image_filtering(\n",
    "            frame,\n",
    "            self.ops[\"fb_post_threshold\"],\n",
    "            self.ops[\"fb_threshold_margin\"],\n",
    "            self.ops[\"med_filt_size\" \"\"],\n",
    "            self.template_filter,\n",
    "        )\n",
    "\n",
    "        # find centers of blobs\n",
    "        xs, ys = find_centers(filt_frame)\n",
    "\n",
    "        if len(xs) == 0:\n",
    "            return []\n",
    "\n",
    "        # merge centers too close to one another\n",
    "        xs, ys = merge_centers(xs, ys, self.ops[\"fb_min_blob_spacing\"])\n",
    "\n",
    "        return list(zip(xs, ys))\n",
    "\n",
    "\n",
    "def image_filtering(\n",
    "    frame,\n",
    "    fb_post_threshold=50,\n",
    "    fb_threshold_margin=50,\n",
    "    med_filt_size=5,\n",
    "    template_filter=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    :param frame: numpy array (image)\n",
    "    :param fb_post_threshold: threshold to apply after template matching\n",
    "    :param fb_threshold_margin: threshold to apply before template matching (arg comes after fb_post_threshold for legacy)\n",
    "    :param med_filt_size: size of median filter kernel\n",
    "    :param template_filter: can take prespecified template for filtering (numpy array)\n",
    "    :return: numpy array (image)\n",
    "    \"\"\"\n",
    "\n",
    "    # apply threshold\n",
    "    threshold = np.median(frame) + fb_threshold_margin\n",
    "    frame = (frame > threshold) * frame\n",
    "\n",
    "    # apply median filter\n",
    "    frame = cv2.medianBlur(frame, med_filt_size)\n",
    "\n",
    "    # template filtering\n",
    "    frame = cv2.matchTemplate(frame, template_filter, cv2.TM_CCOEFF)\n",
    "\n",
    "    # pad frame b/c of template matching\n",
    "    p = (len(template_filter) // 2) + 1\n",
    "    frame = np.pad(frame, (p, 0), \"constant\")\n",
    "\n",
    "    return (frame > fb_post_threshold) * frame\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def find_centers(frame):\n",
    "    \"\"\"\n",
    "    function navigate up gradient to identify local peaks\n",
    "    :param frame: numpy array\n",
    "    :return: (xs, ys) lists of blob centers, indexed by blob number\n",
    "    \"\"\"\n",
    "\n",
    "    # skipping edge pixels\n",
    "    sd = frame.shape\n",
    "    edg = 3\n",
    "    [x, y] = np.where(frame[edg : sd[0] - edg, edg : sd[1] - edg - 1])\n",
    "\n",
    "    x = x + edg - 1\n",
    "    y = y + edg - 1\n",
    "\n",
    "    # intialize lists with item (necessary for legacy numba build, should be able to remove depending on numba version)\n",
    "    xs = [0]\n",
    "    ys = [0]\n",
    "\n",
    "    for j in range(0, len(y)):\n",
    "        if (\n",
    "            (frame[x[j], y[j]] >= frame[x[j] - 1, y[j] - 1])\n",
    "            and (frame[x[j], y[j]] >= frame[x[j] - 1, y[j]])\n",
    "            and (frame[x[j], y[j]] >= frame[x[j] - 1, y[j] + 1])\n",
    "            and (frame[x[j], y[j]] >= frame[x[j], y[j] - 1])\n",
    "            and (frame[x[j], y[j]] >= frame[x[j], y[j] + 1])\n",
    "            and (frame[x[j], y[j]] >= frame[x[j] + 1, y[j] - 1])\n",
    "            and (frame[x[j], y[j]] >= frame[x[j] + 1, y[j]])\n",
    "            and (frame[x[j], y[j]] >= frame[x[j] + 1, y[j] + 1])\n",
    "        ):\n",
    "\n",
    "            # ridge/mesa consolidation code\n",
    "            # find horizontal neighbor pixels of equal value\n",
    "            if frame[x[j], y[j]] == frame[x[j] - 1, y[j]]:\n",
    "                pass\n",
    "\n",
    "            # find horizontal neighbor pixels of equal value\n",
    "            elif frame[x[j], y[j]] == frame[x[j], y[j] - 1]:\n",
    "                pass\n",
    "            else:\n",
    "                xs.append(x[j])\n",
    "                ys.append(y[j])\n",
    "\n",
    "    return xs[1:], ys[1:]\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def merge_centers(xs, ys, fb_min_blob_spacing):\n",
    "    \"\"\"\n",
    "    function to merge adjacent detected peaks\n",
    "    :param xs: list of x locations of blobs, indexed by blob number\n",
    "    :param ys: list of y locations of blobs, indexed by blob number\n",
    "    :param fb_min_blob_spacing: minimum distance within which, adjacent blobs are merged\n",
    "    :return: (xs, ys) lists of blob center coords\n",
    "    \"\"\"\n",
    "\n",
    "    # grab number of centers\n",
    "    numcenters = len(xs)\n",
    "\n",
    "    # code for removing rois\n",
    "    code = 9999\n",
    "\n",
    "    if numcenters != 0:\n",
    "\n",
    "        # calculate distance between all centers\n",
    "        # set all values to dummy high values and greedy alg search for min\n",
    "        dist_matrix = np.ones((numcenters, numcenters)) * 200\n",
    "\n",
    "        # calculate distances\n",
    "        for n in range(numcenters):\n",
    "            for n2 in range(n + 1, numcenters):\n",
    "\n",
    "                # calculate distance\n",
    "                dist = np.sqrt(((xs[n] - xs[n2]) ** 2) + ((ys[n] - ys[n2]) ** 2))\n",
    "\n",
    "                # add to distance matrix\n",
    "                dist_matrix[n, n2] = dist\n",
    "\n",
    "        # greedy alg remove minimum distance till we're small enough\n",
    "        mindist = np.amin(dist_matrix)\n",
    "        while mindist < fb_min_blob_spacing:\n",
    "\n",
    "            # get min distance and min pixel pair\n",
    "            mindist = np.amin(dist_matrix)\n",
    "            minpixelpair = np.argmin(dist_matrix)\n",
    "\n",
    "            # fit linear ndx to larger ndx\n",
    "            c2 = minpixelpair % numcenters\n",
    "\n",
    "            # kill c2\n",
    "            xs[c2] = code\n",
    "            ys[c2] = code\n",
    "\n",
    "            # remove acccording to code in matrix\n",
    "            dist_matrix[:, c2] = code\n",
    "            dist_matrix[c2, :] = code\n",
    "\n",
    "    # remove entries with code\n",
    "    xs = [x for x in xs if x != code]\n",
    "    ys = [y for y in ys if y != code]\n",
    "\n",
    "    return xs, ys\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def gaussian_2d(im_width, sigma):\n",
    "    \"\"\"\n",
    "    function to generate a gaussian blob, centered at middle of numpy array\n",
    "    :param im_width: width of frame\n",
    "    :param sigma: param for gaussian\n",
    "    :return: im_width x im_width numpy array\n",
    "    \"\"\"\n",
    "\n",
    "    g = np.zeros((im_width, im_width), dtype=np.float32)\n",
    "\n",
    "    # gaussian filter\n",
    "    for i in range(int(-(im_width - 1) / 2), int((im_width + 1) / 2)):\n",
    "        for j in range(int(-(im_width - 1) / 2), int((im_width + 1) / 2)):\n",
    "            x0 = int((im_width) / 2)  # center\n",
    "            y0 = int((im_width) / 2)  # center\n",
    "            x = i + x0  # row\n",
    "            y = j + y0  # col\n",
    "            g[y, x] = np.exp(-((x - x0) ** 2 + (y - y0) ** 2) / 2 / sigma / sigma)\n",
    "\n",
    "    return g\n",
    "\n",
    "\n",
    "def build_ops():\n",
    "    \"\"\"\n",
    "    function to pack default parameters for segmentation into a dict\n",
    "    \"\"\"\n",
    "\n",
    "    # quant params\n",
    "    fb_threshold_margin = 40\n",
    "    fb_min_blob_spacing = 8\n",
    "    fb_post_threshold = 40\n",
    "    cb_maxdist = 9\n",
    "    template_filter_width = 3\n",
    "    med_filt_size = 5\n",
    "\n",
    "    ops = {\n",
    "        \"fb_threshold_margin\": fb_threshold_margin,\n",
    "        \"fb_min_blob_spacing\": fb_min_blob_spacing,\n",
    "        \"fb_post_threshold\": fb_post_threshold,\n",
    "        \"cd_maxdist\": cb_maxdist,\n",
    "        \"template_filter_width\": template_filter_width,\n",
    "        \"med_filt_size\": med_filt_size,\n",
    "    }\n",
    "\n",
    "    return ops\n",
    "\n"
   ]
  },
  {
   "source": [
    "define your image processing function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this case we're doing some image processing, finding local peaks, and taking a 3x3 grid of pixel values from each peak\n",
    "# this function returns an average across all detected peaks\n",
    "def process_frame(frame, ip, is_demo=False):\n",
    "\n",
    "    # if we're running this example with the micromanager demo config, peakfinding doesn't really make sense on gratings\n",
    "    if is_demo:\n",
    "        return 0\n",
    "\n",
    "    # simple peakfinding algorithm from accompanying module\n",
    "    xys_list = ip.segmentchunk(frame.astype(np.float32))\n",
    "\n",
    "    # if no peaks, return placeholder value\n",
    "    if len(xys_list) == 0:\n",
    "        return 0\n",
    "\n",
    "    # grab 3x3 pixels around each peak\n",
    "    pix = []\n",
    "    xys = np.array(xys_list) - 1  # -1 because of single pixel offset bug...\n",
    "    pix.append(frame[xys[:, 0], xys[:, 1]])\n",
    "    pix.append(frame[xys[:, 0], xys[:, 1] - 1])\n",
    "    pix.append(frame[xys[:, 0], xys[:, 1] + 1])\n",
    "    pix.append(frame[xys[:, 0] - 1, xys[:, 1]])\n",
    "    pix.append(frame[xys[:, 0] - 1, xys[:, 1] - 1])\n",
    "    pix.append(frame[xys[:, 0] - 1, xys[:, 1] + 1])\n",
    "    pix.append(frame[xys[:, 0] + 1, xys[:, 1]])\n",
    "    pix.append(frame[xys[:, 0] + 1, xys[:, 1] - 1])\n",
    "    pix.append(frame[xys[:, 0] + 1, xys[:, 1] + 1])\n",
    "\n",
    "    # flatten and sort peak-averages\n",
    "    peak_averages = np.sort(np.array(pix).mean(axis=0).flatten())\n",
    "\n",
    "    # in this example let's just average across peaks\n",
    "    avg = peak_averages.mean()\n",
    "\n",
    "    return avg\n",
    "\n",
    "#### quantification settings\n",
    "# initialize jit precompilation with an intial image from the microscope\n",
    "ip = ImageProcessor()\n",
    "core.snap_image()\n",
    "tagged_image = core.get_tagged_image()\n",
    "frame = np.reshape(tagged_image.pix, newshape=[tagged_image.tags['Height'], tagged_image.tags['Width']])\n",
    "garbage = process_frame(frame, ip)\n"
   ]
  },
  {
   "source": [
    "define a function for how your real-time quantified data triggers e.g. a microfluidic solenoid or a laser\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that triggers your perturbation/event based on quantified measurements\n",
    "# in this demo we're extracting average pixel intensities from blobs within image. thus the \"model\" can simply\n",
    "# be a list of values, and our process model function simply detects a rise over the threshold of the latest measurement \n",
    "# however our pycro-manager demo camera returns an image of sinusoidal gratings so peakfinding returns nonsense\n",
    "# so, here is just a placeholder function (it's over 9000 lol)\n",
    "def process_model(model, threshold=9000):\n",
    "\n",
    "    if model[-1] > threshold:\n",
    "        \n",
    "        # code here to do whatever perturbation you want\n",
    "        pass\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "source": [
    "run acquisition. iteratively take frames, quantify, and check for stimulation trigger "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### do acquisition\n",
    "print('beginning acquisition...')\n",
    "t0 = time.time()\n",
    "next_call = time.time()  # updated periodically for when to take next image\n",
    "for f in range(num_frames_to_capture):\n",
    "\n",
    "    # snap image\n",
    "    core.snap_image()\n",
    "    tagged_image = core.get_tagged_image()\n",
    "    \n",
    "    # save acquisition time timestamp\n",
    "    t1 = time.time()\n",
    "    acq_timestamps.append(time.time() - t0)\n",
    "\n",
    "    # pixels by default come out as a 1D array. We can reshape them into an image\n",
    "    frame = np.reshape(tagged_image.pix, newshape=[tagged_image.tags['Height'], tagged_image.tags['Width']])\n",
    "\n",
    "    # quantify image and save processing time timestamp\n",
    "    val = process_frame(frame, ip, is_demo=True) \n",
    "    process_timestamps.append(time.time() - t1)\n",
    "\n",
    "    # store latest value in model and conditionally trigger perturbation\n",
    "    model.append(val)\n",
    "    process_model(model)\n",
    "\n",
    "    # helpful printout to monitor progress\n",
    "    if f%50 == 0:\n",
    "        print('current frame: {}'.format(f))\n",
    "\n",
    "    # wait until we're ready to snap next image. note that in this example, the first few images may exceed the strobe delay as numba jit compiles the relevant python functions\n",
    "    nowtime = time.time()\n",
    "    next_call = next_call + interframe_interval / 1000\n",
    "    if next_call - nowtime < 0:\n",
    "        print(\"warning: strobe delay exceeded inter-frame-interval on frame {}.\".format(f))\n",
    "    else:\n",
    "        time.sleep(next_call - nowtime)\n",
    "\n",
    "bridge.close()\n",
    "print('done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('thanks for reading! hope this was helpful - Ray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}